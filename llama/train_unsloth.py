# -*- coding: utf-8 -*-
"""Alpaca + Llama-3 8b Unsloth 2x faster finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).

**[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2 trillion.**

Use our [Llama-3 8b Instruct](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) notebook for conversational style finetunes.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes
import itertools
import random
import sys
from collections import Counter
from pathlib import Path

from loguru import logger
from tqdm import tqdm
from transformers.trainer_utils import get_last_checkpoint
from wuenlp.impl.UIMANLPStructs import UIMADocument

from utils import seed_everything
from ssc.dataset import xmi_to_llama_samples
from llama.cot import CoTConfigs
from ssc.pipeline import annotate_document, annotate_and_evaluate_files

"""* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc
* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.
* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.
* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.
* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
"""

from unsloth import FastLanguageModel
import torch

from argparse import ArgumentParser

parser = ArgumentParser()
parser.add_argument("--train_folder", type=str, required=True)
parser.add_argument("--test_folder", type=str, required=True)
parser.add_argument("--model_identifier", type=str, required=True)
parser.add_argument("--context_size", type=int, required=True)
parser.add_argument("--random_seed", type=int, required=True)
parser.add_argument("--model", type=str, required=True)
parser.add_argument("--num_train_epochs", type=int, required=True)
parser.add_argument("--cot_config", type=str, required=False, default="no_cot")
args = parser.parse_args()

train_folder = args.train_folder
test_folder = Path(args.test_folder)
model_identifier = args.model_identifier
context_size = args.context_size
random_seed = args.random_seed
model = args.model
cot_config = CoTConfigs[args.cot_config].value
num_train_epochs = args.num_train_epochs

seed_everything(random_seed)

print(args.model)
if (Path(model_identifier) / "adapter_config.json").exists():
    print(f"Model {model_identifier} already exists. Stopping to prevent overwriting.")
    sys.exit(0)
    # raise RuntimeError(f"Model {model_identifier} already exists. Stopping to prevent overwriting.")

logger.add(f"{model_identifier}/train.log")

max_seq_length = context_size  # Choose any! We auto support RoPE Scaling internally!
dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",  # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",  # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",  # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",  # Gemma 2.2x faster!
]  # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj", ],
    lora_alpha=16,
    lora_dropout=0,  # Supports any, but = 0 is optimized
    bias="none",  # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing="unsloth",  # True or "unsloth" for very long context
    random_state=random_seed,
    use_rslora=False,  # We support rank stabilized LoRA
    loftq_config=None,  # And LoftQ
)

print(f"Training model {model}")

"""<a name="Data"></a>
### Data Prep
We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.

**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).

**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!

If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).

For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing).
"""

EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN

from datasets import load_dataset, Dataset

# dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
# dataset = dataset.map(formatting_prompts_func, batched = True,)
train_llama_samples = {"llama_sentences": list(itertools.chain(*[xmi_to_llama_samples(
    file, context_size=context_size, tokenizer=tokenizer, train=True, cot_config=cot_config)["llama_sentences"] for file
                                                                 in
                                                                 tqdm(Path(train_folder).iterdir()) if
                                                                 file.name.endswith(".xmi.zip")]))}

true_string = '<|start_header_id|>assistant<|end_header_id|>\n\nTrue' if cot_config == CoTConfigs.no_cot else "the sentence starts a new scene."
label_distribution = Counter(
    ["border" if true_string in sample else "noborder" for sample in train_llama_samples["llama_sentences"]])
print(label_distribution)
train_llama_samples = {"llama_sentences": [sample for sample in train_llama_samples["llama_sentences"] if
                                           true_string in sample or random.randint(0, 100) < 10]}

print(train_llama_samples["llama_sentences"][:5])

train_dataset = Dataset.from_list(
    [dict(zip(train_llama_samples.keys(), values)) for values in zip(*train_llama_samples.values())])
# train_dataset = train_dataset.map(extract_llama_sentences, batched=True)

test_llama_samples = {"llama_sentences": itertools.chain(*[xmi_to_llama_samples(
    file, context_size=context_size, tokenizer=tokenizer, train=True, cot_config=cot_config)["llama_sentences"] for file
                                                           in
                                                           Path(test_folder).iterdir() if
                                                           file.name.endswith(".xmi.zip")])}

test_dataset = Dataset.from_list(
    [dict(zip(test_llama_samples.keys(), values)) for values in zip(*test_llama_samples.values())])
# test_dataset = test_dataset.map(extract_llama_sentences, batched=True)

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    dataset_text_field="llama_sentences",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,  # Can make training 5x faster for short sequences.
    args=TrainingArguments(
        # per_device_train_batch_size=2,
        # gradient_accumulation_steps=4,
        per_device_train_batch_size=20 if "8b" in model_identifier else 1,
        gradient_accumulation_steps=1 if "8b" in model_identifier else 20,
        warmup_steps=5,
        num_train_epochs=num_train_epochs,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=random_seed,
        output_dir=model_identifier,
    ),
)

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train(resume_from_checkpoint=get_last_checkpoint(model_identifier) is not None)

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's run the model! You can change the instruction and input - leave the output blank!
"""

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

"""<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!
"""

model.save_pretrained(model_identifier)  # Local saving
tokenizer.save_pretrained(model_identifier)  # Local saving